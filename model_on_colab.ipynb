{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "\n"
      ],
      "metadata": {
        "id": "vhSSbwtjDzXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract data in processed_icons.zip file from Google Drive\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "uploaded_zip_path = '/content/drive/My Drive/RM/processed_icons.zip'\n",
        "extracted_dir = '/content/extracted_icons'\n",
        "\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "if not os.listdir(extracted_dir):  # Check if the extraction folder is empty\n",
        "    with zipfile.ZipFile(uploaded_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "print(\"extract data complete\")"
      ],
      "metadata": {
        "id": "LmQ1EG3Ycrnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ckeck number of icon images\n",
        "import os\n",
        "\n",
        "extracted_icons_dir = \"/content/extracted_icons\"\n",
        "png_jpg_count = 0\n",
        "\n",
        "for root, _, files in os.walk(extracted_icons_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            png_jpg_count += 1\n",
        "\n",
        "print(f\"Number of .png or .jpg files in 'extracted_icons': {png_jpg_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgKqYZMX7-51",
        "outputId": "c44c573d-9a3c-4de1-be43-76e1fde6d3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of .png or .jpg files in 'extracted_icons': 1296492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "H0Jx80DjfiRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define dataset class\n",
        "class IconDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, sketch_transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sketch_transform = sketch_transform\n",
        "        self.filepairs = []\n",
        "\n",
        "        # Find paired images (sketch and color icons)\n",
        "        print(\"Finding image pairs...\")\n",
        "        for cls in os.listdir(root_dir):\n",
        "            class_dir = os.path.join(root_dir, cls)\n",
        "            if os.path.isdir(class_dir):\n",
        "                for subfolder in os.listdir(class_dir):\n",
        "                    subfolder_dir = os.path.join(class_dir, subfolder)\n",
        "                    if os.path.isdir(subfolder_dir):\n",
        "                        sketch_path = None\n",
        "                        color_icon_path = None\n",
        "\n",
        "                        for file in os.listdir(subfolder_dir):\n",
        "                            if \"sketch_icon\" in file:\n",
        "                                sketch_path = os.path.join(subfolder_dir, file)\n",
        "                            elif \"color_icon\" in file:\n",
        "                                color_icon_path = os.path.join(subfolder_dir, file)\n",
        "\n",
        "                        if sketch_path and color_icon_path:\n",
        "                            self.filepairs.append((sketch_path, color_icon_path))\n",
        "\n",
        "        print(f\"Found {len(self.filepairs)} valid image pairs.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filepairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sketch_icon_path, color_icon_path = self.filepairs[idx]\n",
        "\n",
        "        # Load images\n",
        "        sketch_icon = Image.open(sketch_icon_path).convert('L')  # Grayscale for sketches\n",
        "        color_icon = Image.open(color_icon_path).convert('RGB')  # RGB for color icons\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.sketch_transform:\n",
        "            sketch_icon = self.sketch_transform(sketch_icon)\n",
        "        if self.transform:\n",
        "            color_icon = self.transform(color_icon)\n",
        "\n",
        "        # Using a dummy label of 0\n",
        "        label = 0\n",
        "        return sketch_icon, color_icon, label\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # RGB normalization\n",
        "])\n",
        "\n",
        "sketch_transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Grayscale normalization\n",
        "])\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "data_dir = \"/content/extracted_icons/processed_icons\"\n",
        "icon_dataset = IconDataset(root_dir=data_dir, transform=transform, sketch_transform=sketch_transform)\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "def split_dataset(dataset, train_split=0.8, val_split=0.1):\n",
        "    train_size = int(len(dataset) * train_split)\n",
        "    val_size = int(len(dataset) * val_split)\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "    return torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(icon_dataset)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4),  # Output a single channel\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling to reduce spatial dimensions\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Set up directories for saving checkpoints and models\n",
        "save_dir = '/content/drive/My Drive/RM/saved_model'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load checkpoints if available\n",
        "def load_checkpoint():\n",
        "    generator_ckpt = os.path.join(save_dir, 'generator_checkpoint.pth')\n",
        "    discriminator_ckpt = os.path.join(save_dir, 'discriminator_checkpoint.pth')\n",
        "\n",
        "    if os.path.exists(generator_ckpt):\n",
        "        generator.load_state_dict(torch.load(generator_ckpt))\n",
        "        print(\"Loaded Generator checkpoint.\")\n",
        "\n",
        "    if os.path.exists(discriminator_ckpt):\n",
        "        discriminator.load_state_dict(torch.load(discriminator_ckpt))\n",
        "        print(\"Loaded Discriminator checkpoint.\")\n",
        "\n",
        "# Load the checkpoint of the model to resume training\n",
        "load_checkpoint()\n",
        "\n",
        "# Training loop with checkpoint saving\n",
        "epochs = 10\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    for i, (sketches, real_images, _) in enumerate(train_loader):\n",
        "        sketches, real_images = sketches.to(device), real_images.to(device)\n",
        "        batch_size = sketches.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "        sketches = sketches.repeat(1, 3, 1, 1)  # Convert 1-channel sketches to 3-channel\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        fake_images = generator(sketches)\n",
        "        real_outputs = discriminator(torch.cat((sketches, real_images), dim=1))  # Real input\n",
        "        fake_outputs = discriminator(torch.cat((sketches, fake_images), dim=1))  # Fake input\n",
        "        d_loss = criterion(real_outputs, real_labels) + criterion(fake_outputs, fake_labels)\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_images = generator(sketches)\n",
        "        outputs = discriminator(torch.cat((sketches, fake_images), dim=1))\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save checkpoints\n",
        "    torch.save(generator.state_dict(), os.path.join(save_dir, 'generator_checkpoint.pth'))\n",
        "    torch.save(discriminator.state_dict(), os.path.join(save_dir, 'discriminator_checkpoint.pth'))\n",
        "    print(f\"Checkpoints saved for epoch {epoch+1}.\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOI7M5-Df4JR",
        "outputId": "60f49ae2-a258-46eb-cd31-6e1b232adbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding image pairs...\n",
            "Found 648246 valid image pairs.\n",
            "Using device: cuda\n",
            "Starting training...\n",
            "Epoch [1/10], Step [10/32413], D Loss: 1.1385, G Loss: 1.1567\n",
            "Epoch [1/10], Step [20/32413], D Loss: 0.7459, G Loss: 2.5479\n",
            "Epoch [1/10], Step [30/32413], D Loss: 0.2542, G Loss: 3.7096\n",
            "Epoch [1/10], Step [40/32413], D Loss: 1.2030, G Loss: 2.6620\n",
            "Epoch [1/10], Step [50/32413], D Loss: 0.7146, G Loss: 1.6203\n",
            "Epoch [1/10], Step [60/32413], D Loss: 0.9042, G Loss: 1.8591\n",
            "Epoch [1/10], Step [70/32413], D Loss: 1.2598, G Loss: 0.2452\n",
            "Epoch [1/10], Step [80/32413], D Loss: 1.0510, G Loss: 0.9623\n",
            "Epoch [1/10], Step [90/32413], D Loss: 1.0960, G Loss: 4.1219\n",
            "Epoch [1/10], Step [100/32413], D Loss: 1.4111, G Loss: 0.5492\n",
            "Epoch [1/10], Step [110/32413], D Loss: 1.0753, G Loss: 1.7104\n",
            "Epoch [1/10], Step [120/32413], D Loss: 0.8812, G Loss: 1.8177\n",
            "Epoch [1/10], Step [130/32413], D Loss: 0.9675, G Loss: 1.0166\n",
            "Epoch [1/10], Step [140/32413], D Loss: 1.1612, G Loss: 0.7356\n",
            "Epoch [1/10], Step [150/32413], D Loss: 0.7647, G Loss: 1.4941\n",
            "Epoch [1/10], Step [160/32413], D Loss: 0.6908, G Loss: 2.6236\n",
            "Epoch [1/10], Step [170/32413], D Loss: 0.6576, G Loss: 1.6401\n",
            "Epoch [1/10], Step [180/32413], D Loss: 0.9680, G Loss: 1.6800\n",
            "Epoch [1/10], Step [190/32413], D Loss: 0.8261, G Loss: 1.3641\n",
            "Epoch [1/10], Step [200/32413], D Loss: 0.5290, G Loss: 2.2073\n",
            "Epoch [1/10], Step [210/32413], D Loss: 0.3069, G Loss: 1.8461\n",
            "Epoch [1/10], Step [220/32413], D Loss: 0.9204, G Loss: 1.0407\n",
            "Epoch [1/10], Step [230/32413], D Loss: 0.5547, G Loss: 2.5151\n",
            "Epoch [1/10], Step [240/32413], D Loss: 1.0895, G Loss: 0.2271\n",
            "Epoch [1/10], Step [250/32413], D Loss: 0.3960, G Loss: 1.7901\n",
            "Epoch [1/10], Step [260/32413], D Loss: 1.4027, G Loss: 0.1620\n",
            "Epoch [1/10], Step [270/32413], D Loss: 0.4342, G Loss: 1.5058\n",
            "Epoch [1/10], Step [280/32413], D Loss: 0.7290, G Loss: 1.3718\n",
            "Epoch [1/10], Step [290/32413], D Loss: 0.2510, G Loss: 2.6524\n",
            "Epoch [1/10], Step [300/32413], D Loss: 0.1304, G Loss: 3.2509\n",
            "Epoch [1/10], Step [310/32413], D Loss: 0.1347, G Loss: 3.1779\n",
            "Epoch [1/10], Step [320/32413], D Loss: 0.1455, G Loss: 3.3078\n",
            "Epoch [1/10], Step [330/32413], D Loss: 0.0844, G Loss: 3.9539\n",
            "Epoch [1/10], Step [340/32413], D Loss: 2.4505, G Loss: 2.0793\n",
            "Epoch [1/10], Step [350/32413], D Loss: 0.8061, G Loss: 0.8473\n",
            "Epoch [1/10], Step [360/32413], D Loss: 0.6639, G Loss: 1.3556\n",
            "Epoch [1/10], Step [370/32413], D Loss: 0.3607, G Loss: 1.8045\n",
            "Epoch [1/10], Step [380/32413], D Loss: 0.9031, G Loss: 1.9835\n",
            "Epoch [1/10], Step [390/32413], D Loss: 0.3748, G Loss: 1.8424\n",
            "Epoch [1/10], Step [400/32413], D Loss: 0.2822, G Loss: 2.5771\n",
            "Epoch [1/10], Step [410/32413], D Loss: 0.3900, G Loss: 3.0706\n",
            "Epoch [1/10], Step [420/32413], D Loss: 0.1494, G Loss: 2.8496\n",
            "Epoch [1/10], Step [430/32413], D Loss: 0.7767, G Loss: 1.2421\n",
            "Epoch [1/10], Step [440/32413], D Loss: 0.3715, G Loss: 2.0794\n",
            "Epoch [1/10], Step [450/32413], D Loss: 0.2754, G Loss: 2.9009\n",
            "Epoch [1/10], Step [460/32413], D Loss: 0.3797, G Loss: 2.3590\n",
            "Epoch [1/10], Step [470/32413], D Loss: 0.2215, G Loss: 2.2101\n",
            "Epoch [1/10], Step [480/32413], D Loss: 0.1495, G Loss: 3.1795\n",
            "Epoch [1/10], Step [490/32413], D Loss: 0.1290, G Loss: 3.1086\n",
            "Epoch [1/10], Step [500/32413], D Loss: 2.1143, G Loss: 2.2709\n",
            "Epoch [1/10], Step [510/32413], D Loss: 0.6406, G Loss: 1.2627\n",
            "Epoch [1/10], Step [520/32413], D Loss: 0.3098, G Loss: 3.0979\n",
            "Epoch [1/10], Step [530/32413], D Loss: 0.4237, G Loss: 3.5135\n",
            "Epoch [1/10], Step [540/32413], D Loss: 0.4217, G Loss: 1.9545\n",
            "Epoch [1/10], Step [550/32413], D Loss: 1.5533, G Loss: 1.3701\n",
            "Epoch [1/10], Step [560/32413], D Loss: 0.6085, G Loss: 1.6907\n",
            "Epoch [1/10], Step [570/32413], D Loss: 0.6502, G Loss: 2.7350\n",
            "Epoch [1/10], Step [580/32413], D Loss: 0.2001, G Loss: 2.6216\n",
            "Epoch [1/10], Step [590/32413], D Loss: 0.0804, G Loss: 3.4239\n",
            "Epoch [1/10], Step [600/32413], D Loss: 0.0940, G Loss: 4.1820\n",
            "Epoch [1/10], Step [610/32413], D Loss: 1.1250, G Loss: 2.6594\n",
            "Epoch [1/10], Step [620/32413], D Loss: 0.4524, G Loss: 2.3788\n",
            "Epoch [1/10], Step [630/32413], D Loss: 0.1497, G Loss: 3.6985\n",
            "Epoch [1/10], Step [640/32413], D Loss: 0.4793, G Loss: 2.6302\n",
            "Epoch [1/10], Step [650/32413], D Loss: 0.1751, G Loss: 2.5871\n",
            "Epoch [1/10], Step [660/32413], D Loss: 0.0816, G Loss: 4.0593\n",
            "Epoch [1/10], Step [670/32413], D Loss: 2.0719, G Loss: 2.7482\n",
            "Epoch [1/10], Step [680/32413], D Loss: 0.5288, G Loss: 2.0033\n",
            "Epoch [1/10], Step [690/32413], D Loss: 0.2878, G Loss: 2.2512\n",
            "Epoch [1/10], Step [700/32413], D Loss: 0.1020, G Loss: 3.2521\n",
            "Epoch [1/10], Step [710/32413], D Loss: 0.1309, G Loss: 3.2073\n",
            "Epoch [1/10], Step [720/32413], D Loss: 0.1784, G Loss: 2.7348\n",
            "Epoch [1/10], Step [730/32413], D Loss: 1.8394, G Loss: 1.7452\n",
            "Epoch [1/10], Step [740/32413], D Loss: 0.4229, G Loss: 3.3698\n",
            "Epoch [1/10], Step [750/32413], D Loss: 0.1692, G Loss: 3.3409\n",
            "Epoch [1/10], Step [760/32413], D Loss: 0.4596, G Loss: 3.0516\n",
            "Epoch [1/10], Step [770/32413], D Loss: 0.3041, G Loss: 2.9237\n",
            "Epoch [1/10], Step [780/32413], D Loss: 0.2498, G Loss: 2.3487\n",
            "Epoch [1/10], Step [790/32413], D Loss: 0.1061, G Loss: 4.0497\n",
            "Epoch [1/10], Step [800/32413], D Loss: 0.1153, G Loss: 3.4332\n",
            "Epoch [1/10], Step [810/32413], D Loss: 0.0561, G Loss: 4.1713\n",
            "Epoch [1/10], Step [820/32413], D Loss: 0.0977, G Loss: 4.9082\n",
            "Epoch [1/10], Step [830/32413], D Loss: 0.1863, G Loss: 3.6260\n",
            "Epoch [1/10], Step [840/32413], D Loss: 0.1113, G Loss: 3.3901\n",
            "Epoch [1/10], Step [850/32413], D Loss: 0.0453, G Loss: 3.4843\n",
            "Epoch [1/10], Step [860/32413], D Loss: 1.1358, G Loss: 2.8260\n",
            "Epoch [1/10], Step [870/32413], D Loss: 0.3721, G Loss: 3.4396\n",
            "Epoch [1/10], Step [880/32413], D Loss: 0.3360, G Loss: 1.8193\n",
            "Epoch [1/10], Step [890/32413], D Loss: 0.6107, G Loss: 1.4243\n",
            "Epoch [1/10], Step [900/32413], D Loss: 0.1250, G Loss: 4.4898\n",
            "Epoch [1/10], Step [910/32413], D Loss: 0.0911, G Loss: 3.9968\n",
            "Epoch [1/10], Step [920/32413], D Loss: 0.0570, G Loss: 4.3699\n",
            "Epoch [1/10], Step [930/32413], D Loss: 0.0569, G Loss: 4.6369\n",
            "Epoch [1/10], Step [940/32413], D Loss: 0.0394, G Loss: 4.6359\n",
            "Epoch [1/10], Step [950/32413], D Loss: 0.0740, G Loss: 4.4259\n",
            "Epoch [1/10], Step [960/32413], D Loss: 0.1196, G Loss: 4.3598\n",
            "Epoch [1/10], Step [970/32413], D Loss: 0.1216, G Loss: 3.9205\n",
            "Epoch [1/10], Step [980/32413], D Loss: 0.1276, G Loss: 4.4879\n",
            "Epoch [1/10], Step [990/32413], D Loss: 0.2400, G Loss: 2.0270\n",
            "Epoch [1/10], Step [1000/32413], D Loss: 0.3114, G Loss: 3.7612\n",
            "Epoch [1/10], Step [1010/32413], D Loss: 0.0164, G Loss: 5.7850\n",
            "Epoch [1/10], Step [1020/32413], D Loss: 0.0175, G Loss: 5.1281\n",
            "Epoch [1/10], Step [1030/32413], D Loss: 0.1372, G Loss: 4.4584\n",
            "Epoch [1/10], Step [1040/32413], D Loss: 0.0711, G Loss: 4.9987\n",
            "Epoch [1/10], Step [1050/32413], D Loss: 0.0699, G Loss: 3.9229\n",
            "Epoch [1/10], Step [1060/32413], D Loss: 0.0304, G Loss: 4.4661\n",
            "Epoch [1/10], Step [1070/32413], D Loss: 0.0085, G Loss: 6.2472\n",
            "Epoch [1/10], Step [1080/32413], D Loss: 1.1060, G Loss: 2.2676\n",
            "Epoch [1/10], Step [1090/32413], D Loss: 0.7844, G Loss: 4.1949\n",
            "Epoch [1/10], Step [1100/32413], D Loss: 0.1155, G Loss: 4.2444\n",
            "Epoch [1/10], Step [1110/32413], D Loss: 0.2798, G Loss: 2.8999\n",
            "Epoch [1/10], Step [1120/32413], D Loss: 0.2406, G Loss: 1.9260\n",
            "Epoch [1/10], Step [1130/32413], D Loss: 0.1023, G Loss: 3.3731\n",
            "Epoch [1/10], Step [1140/32413], D Loss: 0.9269, G Loss: 0.4451\n",
            "Epoch [1/10], Step [1150/32413], D Loss: 0.3086, G Loss: 2.7667\n",
            "Epoch [1/10], Step [1160/32413], D Loss: 0.1432, G Loss: 3.8195\n",
            "Epoch [1/10], Step [1170/32413], D Loss: 0.0985, G Loss: 3.2539\n",
            "Epoch [1/10], Step [1180/32413], D Loss: 0.1013, G Loss: 3.3198\n",
            "Epoch [1/10], Step [1190/32413], D Loss: 0.0289, G Loss: 4.2535\n",
            "Epoch [1/10], Step [1200/32413], D Loss: 0.0200, G Loss: 4.8285\n",
            "Epoch [1/10], Step [1210/32413], D Loss: 0.0283, G Loss: 4.8598\n",
            "Epoch [1/10], Step [1220/32413], D Loss: 0.0202, G Loss: 4.4954\n",
            "Epoch [1/10], Step [1230/32413], D Loss: 0.0202, G Loss: 4.7106\n",
            "Epoch [1/10], Step [1240/32413], D Loss: 0.0094, G Loss: 5.4014\n",
            "Epoch [1/10], Step [1250/32413], D Loss: 0.0290, G Loss: 4.7022\n",
            "Epoch [1/10], Step [1260/32413], D Loss: 0.0188, G Loss: 5.2980\n",
            "Epoch [1/10], Step [1270/32413], D Loss: 0.0172, G Loss: 5.2457\n",
            "Epoch [1/10], Step [1280/32413], D Loss: 0.0562, G Loss: 4.5047\n",
            "Epoch [1/10], Step [1290/32413], D Loss: 0.0594, G Loss: 4.1606\n",
            "Epoch [1/10], Step [1300/32413], D Loss: 0.0730, G Loss: 4.3727\n",
            "Epoch [1/10], Step [1310/32413], D Loss: 0.8423, G Loss: 2.3049\n",
            "Epoch [1/10], Step [1320/32413], D Loss: 0.3167, G Loss: 4.2957\n",
            "Epoch [1/10], Step [1330/32413], D Loss: 0.1672, G Loss: 2.7159\n",
            "Epoch [1/10], Step [1340/32413], D Loss: 0.0499, G Loss: 5.0955\n",
            "Epoch [1/10], Step [1350/32413], D Loss: 0.0084, G Loss: 5.2481\n",
            "Epoch [1/10], Step [1360/32413], D Loss: 0.0314, G Loss: 4.8490\n",
            "Epoch [1/10], Step [1370/32413], D Loss: 0.0228, G Loss: 4.1699\n",
            "Epoch [1/10], Step [1380/32413], D Loss: 0.0155, G Loss: 5.1727\n",
            "Epoch [1/10], Step [1390/32413], D Loss: 0.0293, G Loss: 6.2085\n",
            "Epoch [1/10], Step [1400/32413], D Loss: 0.0088, G Loss: 5.6922\n",
            "Epoch [1/10], Step [1410/32413], D Loss: 0.0615, G Loss: 4.6140\n",
            "Epoch [1/10], Step [1420/32413], D Loss: 0.0377, G Loss: 4.5200\n",
            "Epoch [1/10], Step [1430/32413], D Loss: 0.5598, G Loss: 2.1721\n",
            "Epoch [1/10], Step [1440/32413], D Loss: 0.4689, G Loss: 1.9302\n",
            "Epoch [1/10], Step [1450/32413], D Loss: 0.4609, G Loss: 2.6565\n",
            "Epoch [1/10], Step [1460/32413], D Loss: 0.2647, G Loss: 3.8477\n",
            "Epoch [1/10], Step [1470/32413], D Loss: 0.0511, G Loss: 4.4974\n",
            "Epoch [1/10], Step [1480/32413], D Loss: 0.0626, G Loss: 3.9428\n",
            "Epoch [1/10], Step [1490/32413], D Loss: 0.7740, G Loss: 0.7438\n",
            "Epoch [1/10], Step [1500/32413], D Loss: 0.1641, G Loss: 3.5387\n",
            "Epoch [1/10], Step [1510/32413], D Loss: 0.1157, G Loss: 3.1644\n",
            "Epoch [1/10], Step [1520/32413], D Loss: 0.0916, G Loss: 2.3209\n",
            "Epoch [1/10], Step [1530/32413], D Loss: 0.0477, G Loss: 4.0079\n",
            "Epoch [1/10], Step [1540/32413], D Loss: 0.0414, G Loss: 4.4889\n",
            "Epoch [1/10], Step [1550/32413], D Loss: 0.0361, G Loss: 4.3620\n",
            "Epoch [1/10], Step [1560/32413], D Loss: 0.0297, G Loss: 5.0593\n",
            "Epoch [1/10], Step [1570/32413], D Loss: 0.0335, G Loss: 4.1142\n",
            "Epoch [1/10], Step [1580/32413], D Loss: 0.0282, G Loss: 4.6905\n",
            "Epoch [1/10], Step [1590/32413], D Loss: 0.0234, G Loss: 5.2322\n",
            "Epoch [1/10], Step [1600/32413], D Loss: 0.0278, G Loss: 5.1850\n",
            "Epoch [1/10], Step [1610/32413], D Loss: 0.0175, G Loss: 6.6636\n",
            "Epoch [1/10], Step [1620/32413], D Loss: 0.0147, G Loss: 6.0002\n",
            "Epoch [1/10], Step [1630/32413], D Loss: 0.0738, G Loss: 3.9136\n",
            "Epoch [1/10], Step [1640/32413], D Loss: 0.0127, G Loss: 4.9958\n",
            "Epoch [1/10], Step [1650/32413], D Loss: 0.0141, G Loss: 5.6507\n",
            "Epoch [1/10], Step [1660/32413], D Loss: 0.0115, G Loss: 5.1746\n",
            "Epoch [1/10], Step [1670/32413], D Loss: 0.0461, G Loss: 3.8094\n",
            "Epoch [1/10], Step [1680/32413], D Loss: 0.0200, G Loss: 4.3634\n",
            "Epoch [1/10], Step [1690/32413], D Loss: 0.1197, G Loss: 6.2481\n",
            "Epoch [1/10], Step [1700/32413], D Loss: 0.1628, G Loss: 3.0429\n",
            "Epoch [1/10], Step [1710/32413], D Loss: 0.5907, G Loss: 5.0292\n",
            "Epoch [1/10], Step [1720/32413], D Loss: 0.1994, G Loss: 3.3405\n",
            "Epoch [1/10], Step [1730/32413], D Loss: 0.0348, G Loss: 4.9986\n",
            "Epoch [1/10], Step [1740/32413], D Loss: 0.0508, G Loss: 3.0014\n",
            "Epoch [1/10], Step [1750/32413], D Loss: 0.0559, G Loss: 4.0925\n",
            "Epoch [1/10], Step [1760/32413], D Loss: 0.0786, G Loss: 4.1992\n",
            "Epoch [1/10], Step [1770/32413], D Loss: 0.0925, G Loss: 3.4723\n",
            "Epoch [1/10], Step [1780/32413], D Loss: 0.0135, G Loss: 5.4716\n",
            "Epoch [1/10], Step [1790/32413], D Loss: 0.0498, G Loss: 4.4408\n",
            "Epoch [1/10], Step [1800/32413], D Loss: 1.6333, G Loss: 3.8186\n",
            "Epoch [1/10], Step [1810/32413], D Loss: 0.0440, G Loss: 3.9675\n",
            "Epoch [1/10], Step [1820/32413], D Loss: 0.0233, G Loss: 4.8935\n",
            "Epoch [1/10], Step [1830/32413], D Loss: 0.0756, G Loss: 4.1362\n",
            "Epoch [1/10], Step [1840/32413], D Loss: 0.0360, G Loss: 4.7281\n",
            "Epoch [1/10], Step [1850/32413], D Loss: 0.1547, G Loss: 4.2070\n",
            "Epoch [1/10], Step [1860/32413], D Loss: 0.0283, G Loss: 4.4257\n",
            "Epoch [1/10], Step [1870/32413], D Loss: 0.1133, G Loss: 3.8908\n",
            "Epoch [1/10], Step [1880/32413], D Loss: 0.0190, G Loss: 6.1086\n",
            "Epoch [1/10], Step [1890/32413], D Loss: 0.0098, G Loss: 5.4176\n",
            "Epoch [1/10], Step [1900/32413], D Loss: 0.0213, G Loss: 5.0794\n",
            "Epoch [1/10], Step [1910/32413], D Loss: 0.0066, G Loss: 5.7340\n",
            "Epoch [1/10], Step [1920/32413], D Loss: 0.0194, G Loss: 5.1785\n",
            "Epoch [1/10], Step [1930/32413], D Loss: 0.0392, G Loss: 4.6695\n",
            "Epoch [1/10], Step [1940/32413], D Loss: 0.0109, G Loss: 4.6616\n",
            "Epoch [1/10], Step [1950/32413], D Loss: 0.0062, G Loss: 6.6158\n",
            "Epoch [1/10], Step [1960/32413], D Loss: 0.0121, G Loss: 5.8447\n",
            "Epoch [1/10], Step [1970/32413], D Loss: 0.0130, G Loss: 5.0925\n",
            "Epoch [1/10], Step [1980/32413], D Loss: 0.0237, G Loss: 5.4734\n",
            "Epoch [1/10], Step [1990/32413], D Loss: 0.0039, G Loss: 6.4985\n",
            "Epoch [1/10], Step [2000/32413], D Loss: 0.0023, G Loss: 6.5431\n",
            "Epoch [1/10], Step [2010/32413], D Loss: 0.0090, G Loss: 5.7928\n",
            "Epoch [1/10], Step [2020/32413], D Loss: 0.0084, G Loss: 5.5313\n",
            "Epoch [1/10], Step [2030/32413], D Loss: 0.0024, G Loss: 6.8627\n",
            "Epoch [1/10], Step [2040/32413], D Loss: 0.1167, G Loss: 3.4413\n",
            "Epoch [1/10], Step [2050/32413], D Loss: 0.0505, G Loss: 4.1647\n",
            "Epoch [1/10], Step [2060/32413], D Loss: 0.0546, G Loss: 4.0252\n",
            "Epoch [1/10], Step [2070/32413], D Loss: 0.1305, G Loss: 5.1482\n",
            "Epoch [1/10], Step [2080/32413], D Loss: 0.0112, G Loss: 5.6226\n",
            "Epoch [1/10], Step [2090/32413], D Loss: 0.0024, G Loss: 9.1823\n",
            "Epoch [1/10], Step [2100/32413], D Loss: 0.1129, G Loss: 4.4230\n",
            "Epoch [1/10], Step [2110/32413], D Loss: 0.0148, G Loss: 6.0715\n",
            "Epoch [1/10], Step [2120/32413], D Loss: 0.0118, G Loss: 6.3513\n",
            "Epoch [1/10], Step [2130/32413], D Loss: 0.0181, G Loss: 6.8338\n",
            "Epoch [1/10], Step [2140/32413], D Loss: 0.0401, G Loss: 4.7209\n",
            "Epoch [1/10], Step [2150/32413], D Loss: 0.0421, G Loss: 5.7770\n",
            "Epoch [1/10], Step [2160/32413], D Loss: 0.1534, G Loss: 7.5633\n",
            "Epoch [1/10], Step [2170/32413], D Loss: 0.0194, G Loss: 5.1287\n",
            "Epoch [1/10], Step [2180/32413], D Loss: 0.0406, G Loss: 4.6022\n",
            "Epoch [1/10], Step [2190/32413], D Loss: 0.0061, G Loss: 5.6143\n",
            "Epoch [1/10], Step [2200/32413], D Loss: 0.0031, G Loss: 6.2815\n",
            "Epoch [1/10], Step [2210/32413], D Loss: 0.0037, G Loss: 7.5140\n",
            "Epoch [1/10], Step [2220/32413], D Loss: 0.0015, G Loss: 8.7422\n",
            "Epoch [1/10], Step [2230/32413], D Loss: 0.2052, G Loss: 4.0940\n",
            "Epoch [1/10], Step [2240/32413], D Loss: 0.4561, G Loss: 3.1959\n",
            "Epoch [1/10], Step [2250/32413], D Loss: 0.0234, G Loss: 7.0547\n",
            "Epoch [1/10], Step [2260/32413], D Loss: 0.0317, G Loss: 4.7596\n",
            "Epoch [1/10], Step [2270/32413], D Loss: 0.0099, G Loss: 5.7753\n",
            "Epoch [1/10], Step [2280/32413], D Loss: 0.1304, G Loss: 4.9592\n",
            "Epoch [1/10], Step [2290/32413], D Loss: 0.6656, G Loss: 1.4260\n",
            "Epoch [1/10], Step [2300/32413], D Loss: 0.0198, G Loss: 4.5705\n",
            "Epoch [1/10], Step [2310/32413], D Loss: 0.1041, G Loss: 3.8364\n",
            "Epoch [1/10], Step [2320/32413], D Loss: 0.0107, G Loss: 4.4902\n",
            "Epoch [1/10], Step [2330/32413], D Loss: 0.0326, G Loss: 5.1621\n",
            "Epoch [1/10], Step [2340/32413], D Loss: 0.0201, G Loss: 5.0097\n",
            "Epoch [1/10], Step [2350/32413], D Loss: 0.0174, G Loss: 4.2284\n",
            "Epoch [1/10], Step [2360/32413], D Loss: 0.0045, G Loss: 6.0590\n",
            "Epoch [1/10], Step [2370/32413], D Loss: 0.0358, G Loss: 4.3194\n",
            "Epoch [1/10], Step [2380/32413], D Loss: 0.0172, G Loss: 4.8599\n",
            "Epoch [1/10], Step [2390/32413], D Loss: 0.0099, G Loss: 5.1205\n",
            "Epoch [1/10], Step [2400/32413], D Loss: 0.0099, G Loss: 5.8507\n",
            "Epoch [1/10], Step [2410/32413], D Loss: 0.0103, G Loss: 5.0801\n",
            "Epoch [1/10], Step [2420/32413], D Loss: 0.0024, G Loss: 7.0352\n",
            "Epoch [1/10], Step [2430/32413], D Loss: 0.0041, G Loss: 6.3717\n",
            "Epoch [1/10], Step [2440/32413], D Loss: 0.0403, G Loss: 4.9450\n",
            "Epoch [1/10], Step [2450/32413], D Loss: 0.0214, G Loss: 5.1227\n",
            "Epoch [1/10], Step [2460/32413], D Loss: 0.0061, G Loss: 6.1123\n",
            "Epoch [1/10], Step [2470/32413], D Loss: 0.0346, G Loss: 5.9763\n",
            "Epoch [1/10], Step [2480/32413], D Loss: 0.0049, G Loss: 6.1692\n",
            "Epoch [1/10], Step [2490/32413], D Loss: 0.0068, G Loss: 6.2859\n",
            "Epoch [1/10], Step [2500/32413], D Loss: 0.0055, G Loss: 6.0022\n",
            "Epoch [1/10], Step [2510/32413], D Loss: 0.0038, G Loss: 6.7168\n",
            "Epoch [1/10], Step [2520/32413], D Loss: 0.0027, G Loss: 6.8912\n",
            "Epoch [1/10], Step [2530/32413], D Loss: 0.0014, G Loss: 7.5119\n",
            "Epoch [1/10], Step [2540/32413], D Loss: 0.0108, G Loss: 5.9350\n",
            "Epoch [1/10], Step [2550/32413], D Loss: 0.0023, G Loss: 7.6088\n",
            "Epoch [1/10], Step [2560/32413], D Loss: 0.0020, G Loss: 7.2392\n",
            "Epoch [1/10], Step [2570/32413], D Loss: 0.0033, G Loss: 7.6547\n",
            "Epoch [1/10], Step [2580/32413], D Loss: 0.0013, G Loss: 7.4995\n",
            "Epoch [1/10], Step [2590/32413], D Loss: 0.0114, G Loss: 5.6525\n",
            "Epoch [1/10], Step [2600/32413], D Loss: 0.0017, G Loss: 7.1560\n",
            "Epoch [1/10], Step [2610/32413], D Loss: 0.0025, G Loss: 7.0972\n",
            "Epoch [1/10], Step [2620/32413], D Loss: 0.0025, G Loss: 6.7013\n",
            "Epoch [1/10], Step [2630/32413], D Loss: 0.0011, G Loss: 7.4673\n",
            "Epoch [1/10], Step [2640/32413], D Loss: 0.0114, G Loss: 6.6806\n",
            "Epoch [1/10], Step [2650/32413], D Loss: 0.0033, G Loss: 7.6528\n",
            "Epoch [1/10], Step [2660/32413], D Loss: 0.0062, G Loss: 7.5945\n",
            "Epoch [1/10], Step [2670/32413], D Loss: 0.0017, G Loss: 7.3938\n",
            "Epoch [1/10], Step [2680/32413], D Loss: 0.0016, G Loss: 7.3557\n",
            "Epoch [1/10], Step [2690/32413], D Loss: 0.0090, G Loss: 5.7634\n",
            "Epoch [1/10], Step [2700/32413], D Loss: 0.0014, G Loss: 7.4544\n",
            "Epoch [1/10], Step [2710/32413], D Loss: 0.0025, G Loss: 6.4967\n",
            "Epoch [1/10], Step [2720/32413], D Loss: 0.0012, G Loss: 7.2522\n",
            "Epoch [1/10], Step [2730/32413], D Loss: 0.0019, G Loss: 6.7890\n",
            "Epoch [1/10], Step [2740/32413], D Loss: 0.0059, G Loss: 7.0619\n",
            "Epoch [1/10], Step [2750/32413], D Loss: 0.0014, G Loss: 6.9980\n",
            "Epoch [1/10], Step [2760/32413], D Loss: 0.0020, G Loss: 6.7794\n",
            "Epoch [1/10], Step [2770/32413], D Loss: 0.0135, G Loss: 6.2748\n",
            "Epoch [1/10], Step [2780/32413], D Loss: 0.0018, G Loss: 6.8295\n",
            "Epoch [1/10], Step [2790/32413], D Loss: 0.0019, G Loss: 6.6711\n",
            "Epoch [1/10], Step [2800/32413], D Loss: 0.0016, G Loss: 6.9920\n",
            "Epoch [1/10], Step [2810/32413], D Loss: 0.0035, G Loss: 6.3637\n",
            "Epoch [1/10], Step [2820/32413], D Loss: 0.0023, G Loss: 6.3376\n",
            "Epoch [1/10], Step [2830/32413], D Loss: 0.0014, G Loss: 6.8659\n",
            "Epoch [1/10], Step [2840/32413], D Loss: 0.0041, G Loss: 6.1586\n",
            "Epoch [1/10], Step [2850/32413], D Loss: 0.0026, G Loss: 6.3444\n",
            "Epoch [1/10], Step [2860/32413], D Loss: 0.0025, G Loss: 6.4569\n",
            "Epoch [1/10], Step [2870/32413], D Loss: 0.0083, G Loss: 7.0216\n",
            "Epoch [1/10], Step [2880/32413], D Loss: 0.0026, G Loss: 6.5759\n",
            "Epoch [1/10], Step [2890/32413], D Loss: 0.0030, G Loss: 6.5974\n",
            "Epoch [1/10], Step [2900/32413], D Loss: 0.0025, G Loss: 6.5839\n",
            "Epoch [1/10], Step [2910/32413], D Loss: 0.0012, G Loss: 7.0739\n",
            "Epoch [1/10], Step [2920/32413], D Loss: 0.0013, G Loss: 6.8399\n",
            "Epoch [1/10], Step [2930/32413], D Loss: 0.0017, G Loss: 7.0816\n",
            "Epoch [1/10], Step [2940/32413], D Loss: 0.0015, G Loss: 6.8240\n",
            "Epoch [1/10], Step [2950/32413], D Loss: 0.0013, G Loss: 6.9778\n",
            "Epoch [1/10], Step [2960/32413], D Loss: 0.0010, G Loss: 7.2583\n",
            "Epoch [1/10], Step [2970/32413], D Loss: 0.0013, G Loss: 6.8597\n",
            "Epoch [1/10], Step [2980/32413], D Loss: 0.0017, G Loss: 7.5419\n",
            "Epoch [1/10], Step [2990/32413], D Loss: 0.0010, G Loss: 7.4441\n",
            "Epoch [1/10], Step [3000/32413], D Loss: 0.0009, G Loss: 7.2849\n",
            "Epoch [1/10], Step [3010/32413], D Loss: 0.0009, G Loss: 7.3801\n",
            "Epoch [1/10], Step [3020/32413], D Loss: 0.0006, G Loss: 7.7446\n",
            "Epoch [1/10], Step [3030/32413], D Loss: 0.0011, G Loss: 7.0509\n",
            "Epoch [1/10], Step [3040/32413], D Loss: 0.0010, G Loss: 7.3160\n",
            "Epoch [1/10], Step [3050/32413], D Loss: 0.0010, G Loss: 7.3195\n",
            "Epoch [1/10], Step [3060/32413], D Loss: 0.0013, G Loss: 7.4318\n",
            "Epoch [1/10], Step [3070/32413], D Loss: 0.0007, G Loss: 7.5648\n",
            "Epoch [1/10], Step [3080/32413], D Loss: 0.0010, G Loss: 7.4821\n",
            "Epoch [1/10], Step [3090/32413], D Loss: 0.0007, G Loss: 7.7604\n",
            "Epoch [1/10], Step [3100/32413], D Loss: 0.0008, G Loss: 7.5183\n",
            "Epoch [1/10], Step [3110/32413], D Loss: 0.0007, G Loss: 7.4690\n",
            "Epoch [1/10], Step [3120/32413], D Loss: 0.0009, G Loss: 7.4075\n",
            "Epoch [1/10], Step [3130/32413], D Loss: 0.0009, G Loss: 7.4278\n",
            "Epoch [1/10], Step [3140/32413], D Loss: 0.0012, G Loss: 7.4694\n",
            "Epoch [1/10], Step [3150/32413], D Loss: 0.0006, G Loss: 7.6313\n",
            "Epoch [1/10], Step [3160/32413], D Loss: 0.0007, G Loss: 7.6979\n",
            "Epoch [1/10], Step [3170/32413], D Loss: 0.0009, G Loss: 7.5984\n",
            "Epoch [1/10], Step [3180/32413], D Loss: 0.0007, G Loss: 7.6020\n",
            "Epoch [1/10], Step [3190/32413], D Loss: 0.0011, G Loss: 7.6723\n",
            "Epoch [1/10], Step [3200/32413], D Loss: 0.0006, G Loss: 7.7045\n",
            "Epoch [1/10], Step [3210/32413], D Loss: 0.0007, G Loss: 7.6930\n",
            "Epoch [1/10], Step [3220/32413], D Loss: 0.0005, G Loss: 7.8970\n",
            "Epoch [1/10], Step [3230/32413], D Loss: 0.0006, G Loss: 8.1153\n",
            "Epoch [1/10], Step [3240/32413], D Loss: 0.0008, G Loss: 8.1717\n",
            "Epoch [1/10], Step [3250/32413], D Loss: 0.0005, G Loss: 7.7107\n",
            "Epoch [1/10], Step [3260/32413], D Loss: 0.0011, G Loss: 7.7541\n",
            "Epoch [1/10], Step [3270/32413], D Loss: 0.0005, G Loss: 7.8741\n",
            "Epoch [1/10], Step [3280/32413], D Loss: 0.0005, G Loss: 7.8868\n",
            "Epoch [1/10], Step [3290/32413], D Loss: 0.0005, G Loss: 7.9301\n",
            "Epoch [1/10], Step [3300/32413], D Loss: 0.0005, G Loss: 7.7507\n",
            "Epoch [1/10], Step [3310/32413], D Loss: 0.0005, G Loss: 7.9058\n",
            "Epoch [1/10], Step [3320/32413], D Loss: 0.0004, G Loss: 7.9837\n",
            "Epoch [1/10], Step [3330/32413], D Loss: 0.0005, G Loss: 8.0537\n",
            "Epoch [1/10], Step [3340/32413], D Loss: 0.0003, G Loss: 8.2457\n",
            "Epoch [1/10], Step [3350/32413], D Loss: 0.0003, G Loss: 8.2665\n",
            "Epoch [1/10], Step [3360/32413], D Loss: 0.0004, G Loss: 8.0971\n",
            "Epoch [1/10], Step [3370/32413], D Loss: 0.0003, G Loss: 8.4696\n",
            "Epoch [1/10], Step [3380/32413], D Loss: 0.0004, G Loss: 8.2166\n",
            "Epoch [1/10], Step [3390/32413], D Loss: 0.0004, G Loss: 8.3054\n",
            "Epoch [1/10], Step [3400/32413], D Loss: 0.0003, G Loss: 8.6064\n",
            "Epoch [1/10], Step [3410/32413], D Loss: 0.0003, G Loss: 8.4879\n",
            "Epoch [1/10], Step [3420/32413], D Loss: 0.0003, G Loss: 8.3523\n",
            "Epoch [1/10], Step [3430/32413], D Loss: 0.0007, G Loss: 8.2569\n",
            "Epoch [1/10], Step [3440/32413], D Loss: 0.0007, G Loss: 8.3565\n",
            "Epoch [1/10], Step [3450/32413], D Loss: 0.0004, G Loss: 8.4323\n",
            "Epoch [1/10], Step [3460/32413], D Loss: 0.0004, G Loss: 8.6071\n",
            "Epoch [1/10], Step [3470/32413], D Loss: 0.0003, G Loss: 8.6259\n",
            "Epoch [1/10], Step [3480/32413], D Loss: 0.0004, G Loss: 8.4731\n",
            "Epoch [1/10], Step [3490/32413], D Loss: 0.0004, G Loss: 8.2596\n",
            "Epoch [1/10], Step [3500/32413], D Loss: 0.0004, G Loss: 8.4466\n",
            "Epoch [1/10], Step [3510/32413], D Loss: 0.0002, G Loss: 8.6574\n",
            "Epoch [1/10], Step [3520/32413], D Loss: 0.0003, G Loss: 8.3845\n",
            "Epoch [1/10], Step [3530/32413], D Loss: 0.0005, G Loss: 8.9031\n",
            "Epoch [1/10], Step [3540/32413], D Loss: 0.0002, G Loss: 8.7164\n",
            "Epoch [1/10], Step [3550/32413], D Loss: 0.0003, G Loss: 8.7529\n",
            "Epoch [1/10], Step [3560/32413], D Loss: 0.0003, G Loss: 8.5430\n",
            "Epoch [1/10], Step [3570/32413], D Loss: 0.0002, G Loss: 8.6228\n",
            "Epoch [1/10], Step [3580/32413], D Loss: 0.0002, G Loss: 8.5143\n",
            "Epoch [1/10], Step [3590/32413], D Loss: 0.0003, G Loss: 8.5725\n",
            "Epoch [1/10], Step [3600/32413], D Loss: 0.0002, G Loss: 8.9064\n",
            "Epoch [1/10], Step [3610/32413], D Loss: 0.0003, G Loss: 8.4729\n",
            "Epoch [1/10], Step [3620/32413], D Loss: 0.0003, G Loss: 8.6061\n",
            "Epoch [1/10], Step [3630/32413], D Loss: 0.0003, G Loss: 9.0137\n",
            "Epoch [1/10], Step [3640/32413], D Loss: 0.0004, G Loss: 8.6519\n",
            "Epoch [1/10], Step [3650/32413], D Loss: 0.0026, G Loss: 8.9678\n",
            "Epoch [1/10], Step [3660/32413], D Loss: 0.0002, G Loss: 9.0445\n",
            "Epoch [1/10], Step [3670/32413], D Loss: 0.0002, G Loss: 9.0485\n",
            "Epoch [1/10], Step [3680/32413], D Loss: 0.0002, G Loss: 8.8000\n",
            "Epoch [1/10], Step [3690/32413], D Loss: 0.0002, G Loss: 8.6097\n",
            "Epoch [1/10], Step [3700/32413], D Loss: 0.0002, G Loss: 9.0057\n",
            "Epoch [1/10], Step [3710/32413], D Loss: 0.0003, G Loss: 8.6390\n",
            "Epoch [1/10], Step [3720/32413], D Loss: 0.0002, G Loss: 8.8464\n",
            "Epoch [1/10], Step [3730/32413], D Loss: 0.0002, G Loss: 8.9034\n",
            "Epoch [1/10], Step [3740/32413], D Loss: 0.0002, G Loss: 9.0746\n",
            "Epoch [1/10], Step [3750/32413], D Loss: 0.0002, G Loss: 8.9273\n",
            "Epoch [1/10], Step [3760/32413], D Loss: 0.0002, G Loss: 8.9051\n",
            "Epoch [1/10], Step [3770/32413], D Loss: 0.0002, G Loss: 8.6547\n",
            "Epoch [1/10], Step [3780/32413], D Loss: 0.0002, G Loss: 8.6668\n",
            "Epoch [1/10], Step [3790/32413], D Loss: 0.0003, G Loss: 8.6851\n",
            "Epoch [1/10], Step [3800/32413], D Loss: 0.0001, G Loss: 9.0938\n",
            "Epoch [1/10], Step [3810/32413], D Loss: 0.0002, G Loss: 8.8966\n",
            "Epoch [1/10], Step [3820/32413], D Loss: 0.0002, G Loss: 8.9109\n",
            "Epoch [1/10], Step [3830/32413], D Loss: 0.0002, G Loss: 8.6340\n",
            "Epoch [1/10], Step [3840/32413], D Loss: 0.0002, G Loss: 9.0181\n",
            "Epoch [1/10], Step [3850/32413], D Loss: 0.0002, G Loss: 8.8101\n",
            "Epoch [1/10], Step [3860/32413], D Loss: 0.0002, G Loss: 8.5767\n",
            "Epoch [1/10], Step [3870/32413], D Loss: 0.0001, G Loss: 9.0977\n",
            "Epoch [1/10], Step [3880/32413], D Loss: 0.0002, G Loss: 9.2403\n",
            "Epoch [1/10], Step [3890/32413], D Loss: 0.0002, G Loss: 8.9924\n",
            "Epoch [1/10], Step [3900/32413], D Loss: 0.0002, G Loss: 8.8753\n",
            "Epoch [1/10], Step [3910/32413], D Loss: 0.0001, G Loss: 9.2825\n",
            "Epoch [1/10], Step [3920/32413], D Loss: 0.0002, G Loss: 8.9251\n",
            "Epoch [1/10], Step [3930/32413], D Loss: 0.0002, G Loss: 8.8364\n",
            "Epoch [1/10], Step [3940/32413], D Loss: 0.0001, G Loss: 9.3589\n",
            "Epoch [1/10], Step [3950/32413], D Loss: 0.0002, G Loss: 8.9238\n",
            "Epoch [1/10], Step [3960/32413], D Loss: 0.0002, G Loss: 9.0132\n",
            "Epoch [1/10], Step [3970/32413], D Loss: 0.0002, G Loss: 8.9982\n",
            "Epoch [1/10], Step [3980/32413], D Loss: 0.0003, G Loss: 8.9984\n",
            "Epoch [1/10], Step [3990/32413], D Loss: 0.0002, G Loss: 9.2980\n",
            "Epoch [1/10], Step [4000/32413], D Loss: 0.0002, G Loss: 9.0883\n",
            "Epoch [1/10], Step [4010/32413], D Loss: 0.0001, G Loss: 9.3322\n",
            "Epoch [1/10], Step [4020/32413], D Loss: 0.0002, G Loss: 8.9221\n",
            "Epoch [1/10], Step [4030/32413], D Loss: 0.0001, G Loss: 9.4050\n",
            "Epoch [1/10], Step [4040/32413], D Loss: 0.0001, G Loss: 9.2289\n",
            "Epoch [1/10], Step [4050/32413], D Loss: 0.0015, G Loss: 9.6099\n",
            "Epoch [1/10], Step [4060/32413], D Loss: 0.0001, G Loss: 9.2545\n",
            "Epoch [1/10], Step [4070/32413], D Loss: 0.0002, G Loss: 8.9405\n",
            "Epoch [1/10], Step [4080/32413], D Loss: 0.0003, G Loss: 9.3312\n",
            "Epoch [1/10], Step [4090/32413], D Loss: 0.0001, G Loss: 8.9207\n",
            "Epoch [1/10], Step [4100/32413], D Loss: 0.0002, G Loss: 8.7839\n",
            "Epoch [1/10], Step [4110/32413], D Loss: 0.0002, G Loss: 8.9757\n",
            "Epoch [1/10], Step [4120/32413], D Loss: 0.0004, G Loss: 9.1352\n",
            "Epoch [1/10], Step [4130/32413], D Loss: 0.0002, G Loss: 8.9372\n",
            "Epoch [1/10], Step [4140/32413], D Loss: 0.0001, G Loss: 9.4090\n",
            "Epoch [1/10], Step [4150/32413], D Loss: 0.0001, G Loss: 9.1021\n",
            "Epoch [1/10], Step [4160/32413], D Loss: 0.0001, G Loss: 9.3058\n",
            "Epoch [1/10], Step [4170/32413], D Loss: 0.0002, G Loss: 9.2220\n",
            "Epoch [1/10], Step [4180/32413], D Loss: 0.0001, G Loss: 9.0684\n",
            "Epoch [1/10], Step [4190/32413], D Loss: 0.0001, G Loss: 9.2946\n",
            "Epoch [1/10], Step [4200/32413], D Loss: 0.0003, G Loss: 9.1002\n",
            "Epoch [1/10], Step [4210/32413], D Loss: 0.0001, G Loss: 9.1249\n",
            "Epoch [1/10], Step [4220/32413], D Loss: 0.0001, G Loss: 9.3583\n",
            "Epoch [1/10], Step [4230/32413], D Loss: 0.0002, G Loss: 9.3303\n",
            "Epoch [1/10], Step [4240/32413], D Loss: 0.0003, G Loss: 9.1080\n",
            "Epoch [1/10], Step [4250/32413], D Loss: 0.0001, G Loss: 9.1467\n",
            "Epoch [1/10], Step [4260/32413], D Loss: 0.0001, G Loss: 9.4976\n",
            "Epoch [1/10], Step [4270/32413], D Loss: 0.0001, G Loss: 9.4130\n",
            "Epoch [1/10], Step [4280/32413], D Loss: 0.0001, G Loss: 9.3492\n",
            "Epoch [1/10], Step [4290/32413], D Loss: 0.0001, G Loss: 9.7214\n",
            "Epoch [1/10], Step [4300/32413], D Loss: 0.0001, G Loss: 9.2914\n",
            "Epoch [1/10], Step [4310/32413], D Loss: 0.0002, G Loss: 9.1777\n",
            "Epoch [1/10], Step [4320/32413], D Loss: 0.0001, G Loss: 9.2729\n",
            "Epoch [1/10], Step [4330/32413], D Loss: 0.0001, G Loss: 9.2352\n",
            "Epoch [1/10], Step [4340/32413], D Loss: 0.0002, G Loss: 9.5645\n",
            "Epoch [1/10], Step [4350/32413], D Loss: 0.0004, G Loss: 9.7926\n",
            "Epoch [1/10], Step [4360/32413], D Loss: 0.0002, G Loss: 8.9673\n",
            "Epoch [1/10], Step [4370/32413], D Loss: 0.0002, G Loss: 8.8588\n",
            "Epoch [1/10], Step [4380/32413], D Loss: 0.0002, G Loss: 9.4257\n",
            "Epoch [1/10], Step [4390/32413], D Loss: 0.0005, G Loss: 10.2689\n",
            "Epoch [1/10], Step [4400/32413], D Loss: 0.0001, G Loss: 9.5022\n",
            "Epoch [1/10], Step [4410/32413], D Loss: 0.0002, G Loss: 9.1169\n",
            "Epoch [1/10], Step [4420/32413], D Loss: 0.0001, G Loss: 9.3553\n",
            "Epoch [1/10], Step [4430/32413], D Loss: 0.0002, G Loss: 9.3771\n",
            "Epoch [1/10], Step [4440/32413], D Loss: 0.0001, G Loss: 9.5754\n",
            "Epoch [1/10], Step [4450/32413], D Loss: 0.0003, G Loss: 9.4055\n",
            "Epoch [1/10], Step [4460/32413], D Loss: 0.0001, G Loss: 9.7696\n",
            "Epoch [1/10], Step [4470/32413], D Loss: 0.0001, G Loss: 9.2676\n",
            "Epoch [1/10], Step [4480/32413], D Loss: 0.0001, G Loss: 9.3875\n",
            "Epoch [1/10], Step [4490/32413], D Loss: 0.0001, G Loss: 9.5838\n",
            "Epoch [1/10], Step [4500/32413], D Loss: 0.0001, G Loss: 9.3690\n",
            "Epoch [1/10], Step [4510/32413], D Loss: 0.0001, G Loss: 9.5198\n",
            "Epoch [1/10], Step [4520/32413], D Loss: 0.0002, G Loss: 9.3283\n",
            "Epoch [1/10], Step [4530/32413], D Loss: 0.0001, G Loss: 9.6719\n",
            "Epoch [1/10], Step [4540/32413], D Loss: 0.0002, G Loss: 9.4540\n",
            "Epoch [1/10], Step [4550/32413], D Loss: 0.0002, G Loss: 9.3035\n",
            "Epoch [1/10], Step [4560/32413], D Loss: 0.0001, G Loss: 9.6636\n",
            "Epoch [1/10], Step [4570/32413], D Loss: 0.0001, G Loss: 9.2707\n",
            "Epoch [1/10], Step [4580/32413], D Loss: 0.0001, G Loss: 9.3087\n",
            "Epoch [1/10], Step [4590/32413], D Loss: 0.0001, G Loss: 9.2307\n",
            "Epoch [1/10], Step [4600/32413], D Loss: 0.0001, G Loss: 9.4823\n",
            "Epoch [1/10], Step [4610/32413], D Loss: 0.0001, G Loss: 9.0657\n",
            "Epoch [1/10], Step [4620/32413], D Loss: 0.0001, G Loss: 9.7121\n",
            "Epoch [1/10], Step [4630/32413], D Loss: 0.0001, G Loss: 9.4014\n",
            "Epoch [1/10], Step [4640/32413], D Loss: 0.0001, G Loss: 9.9350\n",
            "Epoch [1/10], Step [4650/32413], D Loss: 0.0001, G Loss: 9.3525\n",
            "Epoch [1/10], Step [4660/32413], D Loss: 0.0002, G Loss: 9.2261\n",
            "Epoch [1/10], Step [4670/32413], D Loss: 0.0001, G Loss: 9.5354\n",
            "Epoch [1/10], Step [4680/32413], D Loss: 0.0002, G Loss: 9.4259\n",
            "Epoch [1/10], Step [4690/32413], D Loss: 0.0002, G Loss: 8.7478\n",
            "Epoch [1/10], Step [4700/32413], D Loss: 0.0001, G Loss: 9.9303\n",
            "Epoch [1/10], Step [4710/32413], D Loss: 0.0006, G Loss: 9.6427\n",
            "Epoch [1/10], Step [4720/32413], D Loss: 0.0003, G Loss: 8.8723\n",
            "Epoch [1/10], Step [4730/32413], D Loss: 0.0001, G Loss: 9.5494\n",
            "Epoch [1/10], Step [4740/32413], D Loss: 0.0003, G Loss: 8.6046\n",
            "Epoch [1/10], Step [4750/32413], D Loss: 0.0002, G Loss: 10.0176\n",
            "Epoch [1/10], Step [4760/32413], D Loss: 0.0001, G Loss: 9.1253\n",
            "Epoch [1/10], Step [4770/32413], D Loss: 0.0001, G Loss: 9.8605\n",
            "Epoch [1/10], Step [4780/32413], D Loss: 0.0002, G Loss: 8.6747\n",
            "Epoch [1/10], Step [4790/32413], D Loss: 0.0001, G Loss: 9.3027\n",
            "Epoch [1/10], Step [4800/32413], D Loss: 0.0001, G Loss: 9.4998\n",
            "Epoch [1/10], Step [4810/32413], D Loss: 0.0006, G Loss: 8.7634\n",
            "Epoch [1/10], Step [4820/32413], D Loss: 0.0001, G Loss: 9.4001\n",
            "Epoch [1/10], Step [4830/32413], D Loss: 0.0001, G Loss: 9.2120\n",
            "Epoch [1/10], Step [4840/32413], D Loss: 0.0003, G Loss: 8.3143\n",
            "Epoch [1/10], Step [4850/32413], D Loss: 0.0006, G Loss: 7.8151\n",
            "Epoch [1/10], Step [4860/32413], D Loss: 0.0003, G Loss: 9.5605\n",
            "Epoch [1/10], Step [4870/32413], D Loss: 0.0001, G Loss: 9.5519\n",
            "Epoch [1/10], Step [4880/32413], D Loss: 0.0000, G Loss: 10.5583\n",
            "Epoch [1/10], Step [4890/32413], D Loss: 0.0001, G Loss: 9.5366\n",
            "Epoch [1/10], Step [4900/32413], D Loss: 0.0001, G Loss: 9.8008\n",
            "Epoch [1/10], Step [4910/32413], D Loss: 0.0002, G Loss: 9.0816\n",
            "Epoch [1/10], Step [4920/32413], D Loss: 0.0002, G Loss: 8.5890\n",
            "Epoch [1/10], Step [4930/32413], D Loss: 0.0002, G Loss: 8.7382\n",
            "Epoch [1/10], Step [4940/32413], D Loss: 0.0001, G Loss: 10.2502\n",
            "Epoch [1/10], Step [4950/32413], D Loss: 0.0001, G Loss: 11.9549\n",
            "Epoch [1/10], Step [4960/32413], D Loss: 0.0011, G Loss: 8.8850\n",
            "Epoch [1/10], Step [4970/32413], D Loss: 0.0176, G Loss: 8.3264\n",
            "Epoch [1/10], Step [4980/32413], D Loss: 0.0006, G Loss: 7.9609\n",
            "Epoch [1/10], Step [4990/32413], D Loss: 0.0001, G Loss: 9.7409\n",
            "Epoch [1/10], Step [5000/32413], D Loss: 0.0042, G Loss: 7.5435\n",
            "Epoch [1/10], Step [5010/32413], D Loss: 0.0040, G Loss: 8.9455\n",
            "Epoch [1/10], Step [5020/32413], D Loss: 0.0001, G Loss: 8.8673\n",
            "Epoch [1/10], Step [5030/32413], D Loss: 0.0050, G Loss: 6.6891\n",
            "Epoch [1/10], Step [5040/32413], D Loss: 0.0005, G Loss: 8.6833\n",
            "Epoch [1/10], Step [5050/32413], D Loss: 0.0019, G Loss: 7.6793\n",
            "Epoch [1/10], Step [5060/32413], D Loss: 0.0015, G Loss: 9.0568\n",
            "Epoch [1/10], Step [5070/32413], D Loss: 1.5654, G Loss: 1.7970\n",
            "Epoch [1/10], Step [5080/32413], D Loss: 1.3176, G Loss: 0.6742\n",
            "Epoch [1/10], Step [5090/32413], D Loss: 1.4266, G Loss: 0.7741\n",
            "Epoch [1/10], Step [5100/32413], D Loss: 1.0998, G Loss: 0.8631\n",
            "Epoch [1/10], Step [5110/32413], D Loss: 1.1222, G Loss: 0.7409\n",
            "Epoch [1/10], Step [5120/32413], D Loss: 1.0381, G Loss: 0.9714\n",
            "Epoch [1/10], Step [5130/32413], D Loss: 1.3767, G Loss: 1.0939\n",
            "Epoch [1/10], Step [5140/32413], D Loss: 1.0655, G Loss: 1.0748\n",
            "Epoch [1/10], Step [5150/32413], D Loss: 0.8282, G Loss: 1.2266\n",
            "Epoch [1/10], Step [5160/32413], D Loss: 0.8093, G Loss: 1.3368\n",
            "Epoch [1/10], Step [5170/32413], D Loss: 0.6677, G Loss: 1.3227\n",
            "Epoch [1/10], Step [5180/32413], D Loss: 0.7922, G Loss: 1.1876\n",
            "Epoch [1/10], Step [5190/32413], D Loss: 0.7993, G Loss: 2.0548\n",
            "Epoch [1/10], Step [5200/32413], D Loss: 1.4800, G Loss: 0.6057\n",
            "Epoch [1/10], Step [5210/32413], D Loss: 0.5620, G Loss: 1.6737\n",
            "Epoch [1/10], Step [5220/32413], D Loss: 0.5835, G Loss: 1.8796\n",
            "Epoch [1/10], Step [5230/32413], D Loss: 1.4512, G Loss: 1.6763\n",
            "Epoch [1/10], Step [5240/32413], D Loss: 0.2778, G Loss: 2.2403\n",
            "Epoch [1/10], Step [5250/32413], D Loss: 0.3723, G Loss: 2.4111\n",
            "Epoch [1/10], Step [5260/32413], D Loss: 0.4904, G Loss: 2.0310\n",
            "Epoch [1/10], Step [5270/32413], D Loss: 0.5135, G Loss: 2.1653\n",
            "Epoch [1/10], Step [5280/32413], D Loss: 10.3808, G Loss: 1.4954\n",
            "Epoch [1/10], Step [5290/32413], D Loss: 0.4337, G Loss: 2.4175\n",
            "Epoch [1/10], Step [5300/32413], D Loss: 0.1984, G Loss: 2.6210\n",
            "Epoch [1/10], Step [5310/32413], D Loss: 0.6673, G Loss: 5.4666\n",
            "Epoch [1/10], Step [5320/32413], D Loss: 0.9116, G Loss: 3.2589\n",
            "Epoch [1/10], Step [5330/32413], D Loss: 0.6444, G Loss: 3.1429\n",
            "Epoch [1/10], Step [5340/32413], D Loss: 0.7517, G Loss: 4.2304\n",
            "Epoch [1/10], Step [5350/32413], D Loss: 2.5851, G Loss: 1.7174\n",
            "Epoch [1/10], Step [5360/32413], D Loss: 1.0867, G Loss: 3.3521\n",
            "Epoch [1/10], Step [5370/32413], D Loss: 0.3133, G Loss: 1.9980\n",
            "Epoch [1/10], Step [5380/32413], D Loss: 0.3108, G Loss: 3.2736\n",
            "Epoch [1/10], Step [5390/32413], D Loss: 0.1518, G Loss: 3.6228\n",
            "Epoch [1/10], Step [5400/32413], D Loss: 0.1686, G Loss: 4.3529\n",
            "Epoch [1/10], Step [5410/32413], D Loss: 0.3025, G Loss: 4.0821\n",
            "Epoch [1/10], Step [5420/32413], D Loss: 0.0681, G Loss: 3.8215\n",
            "Epoch [1/10], Step [5430/32413], D Loss: 0.0447, G Loss: 4.0270\n",
            "Epoch [1/10], Step [5440/32413], D Loss: 0.0719, G Loss: 3.9813\n",
            "Epoch [1/10], Step [5450/32413], D Loss: 0.0376, G Loss: 4.3239\n",
            "Epoch [1/10], Step [5460/32413], D Loss: 0.0571, G Loss: 4.0493\n",
            "Epoch [1/10], Step [5470/32413], D Loss: 0.0167, G Loss: 4.7424\n",
            "Epoch [1/10], Step [5480/32413], D Loss: 0.0078, G Loss: 7.2909\n",
            "Epoch [1/10], Step [5490/32413], D Loss: 1.5318, G Loss: 2.6278\n",
            "Epoch [1/10], Step [5500/32413], D Loss: 0.7858, G Loss: 1.8115\n",
            "Epoch [1/10], Step [5510/32413], D Loss: 0.4126, G Loss: 2.0958\n",
            "Epoch [1/10], Step [5520/32413], D Loss: 0.5207, G Loss: 2.5731\n",
            "Epoch [1/10], Step [5530/32413], D Loss: 1.0152, G Loss: 2.7333\n",
            "Epoch [1/10], Step [5540/32413], D Loss: 1.0690, G Loss: 3.4263\n",
            "Epoch [1/10], Step [5550/32413], D Loss: 0.1792, G Loss: 2.7509\n",
            "Epoch [1/10], Step [5560/32413], D Loss: 0.4870, G Loss: 2.9461\n",
            "Epoch [1/10], Step [5570/32413], D Loss: 0.3550, G Loss: 1.2450\n",
            "Epoch [1/10], Step [5580/32413], D Loss: 0.1831, G Loss: 3.2438\n",
            "Epoch [1/10], Step [5590/32413], D Loss: 0.0930, G Loss: 3.3280\n",
            "Epoch [1/10], Step [5600/32413], D Loss: 0.1070, G Loss: 2.8052\n",
            "Epoch [1/10], Step [5610/32413], D Loss: 0.0975, G Loss: 4.1609\n",
            "Epoch [1/10], Step [5620/32413], D Loss: 0.0376, G Loss: 4.3799\n",
            "Epoch [1/10], Step [5630/32413], D Loss: 0.0746, G Loss: 4.4106\n",
            "Epoch [1/10], Step [5640/32413], D Loss: 0.0269, G Loss: 5.5262\n",
            "Epoch [1/10], Step [5650/32413], D Loss: 0.0358, G Loss: 3.4444\n",
            "Epoch [1/10], Step [5660/32413], D Loss: 0.6205, G Loss: 1.6726\n",
            "Epoch [1/10], Step [5670/32413], D Loss: 0.8919, G Loss: 2.4805\n",
            "Epoch [1/10], Step [5680/32413], D Loss: 0.1266, G Loss: 2.8243\n",
            "Epoch [1/10], Step [5690/32413], D Loss: 0.0633, G Loss: 3.5073\n",
            "Epoch [1/10], Step [5700/32413], D Loss: 0.0258, G Loss: 4.5439\n",
            "Epoch [1/10], Step [5710/32413], D Loss: 0.9026, G Loss: 0.1995\n",
            "Epoch [1/10], Step [5720/32413], D Loss: 0.4183, G Loss: 3.0514\n",
            "Epoch [1/10], Step [5730/32413], D Loss: 0.0476, G Loss: 3.6374\n",
            "Epoch [1/10], Step [5740/32413], D Loss: 0.0265, G Loss: 4.7645\n",
            "Epoch [1/10], Step [5750/32413], D Loss: 0.0621, G Loss: 3.6573\n",
            "Epoch [1/10], Step [5760/32413], D Loss: 0.3957, G Loss: 1.9450\n",
            "Epoch [1/10], Step [5770/32413], D Loss: 0.1061, G Loss: 3.8279\n",
            "Epoch [1/10], Step [5780/32413], D Loss: 0.0116, G Loss: 5.1130\n",
            "Epoch [1/10], Step [5790/32413], D Loss: 0.0092, G Loss: 5.9667\n",
            "Epoch [1/10], Step [5800/32413], D Loss: 0.8007, G Loss: 1.5472\n",
            "Epoch [1/10], Step [5810/32413], D Loss: 0.0849, G Loss: 5.0110\n",
            "Epoch [1/10], Step [5820/32413], D Loss: 0.1336, G Loss: 3.6065\n",
            "Epoch [1/10], Step [5830/32413], D Loss: 0.4319, G Loss: 6.7297\n",
            "Epoch [1/10], Step [5840/32413], D Loss: 0.0447, G Loss: 3.0584\n",
            "Epoch [1/10], Step [5850/32413], D Loss: 0.0168, G Loss: 6.6168\n",
            "Epoch [1/10], Step [5860/32413], D Loss: 0.0811, G Loss: 3.3871\n",
            "Epoch [1/10], Step [5870/32413], D Loss: 0.0077, G Loss: 5.1835\n",
            "Epoch [1/10], Step [5880/32413], D Loss: 0.0131, G Loss: 4.8962\n",
            "Epoch [1/10], Step [5890/32413], D Loss: 0.1796, G Loss: 5.0047\n",
            "Epoch [1/10], Step [5900/32413], D Loss: 0.1811, G Loss: 3.4384\n",
            "Epoch [1/10], Step [5910/32413], D Loss: 0.0334, G Loss: 6.8270\n",
            "Epoch [1/10], Step [5920/32413], D Loss: 0.0088, G Loss: 5.2909\n",
            "Epoch [1/10], Step [5930/32413], D Loss: 0.0103, G Loss: 4.9959\n",
            "Epoch [1/10], Step [5940/32413], D Loss: 0.0246, G Loss: 4.9524\n",
            "Epoch [1/10], Step [5950/32413], D Loss: 0.0030, G Loss: 6.1731\n",
            "Epoch [1/10], Step [5960/32413], D Loss: 0.0045, G Loss: 7.1035\n",
            "Epoch [1/10], Step [5970/32413], D Loss: 0.0081, G Loss: 5.8255\n",
            "Epoch [1/10], Step [5980/32413], D Loss: 0.0297, G Loss: 7.5712\n",
            "Epoch [1/10], Step [5990/32413], D Loss: 0.0023, G Loss: 6.3827\n",
            "Epoch [1/10], Step [6000/32413], D Loss: 0.0073, G Loss: 5.6615\n",
            "Epoch [1/10], Step [6010/32413], D Loss: 0.0009, G Loss: 7.5235\n",
            "Epoch [1/10], Step [6020/32413], D Loss: 0.0042, G Loss: 6.1422\n",
            "Epoch [1/10], Step [6030/32413], D Loss: 0.0085, G Loss: 5.9381\n",
            "Epoch [1/10], Step [6040/32413], D Loss: 0.0168, G Loss: 5.1873\n",
            "Epoch [1/10], Step [6050/32413], D Loss: 0.0092, G Loss: 6.4847\n",
            "Epoch [1/10], Step [6060/32413], D Loss: 0.0170, G Loss: 6.0219\n",
            "Epoch [1/10], Step [6070/32413], D Loss: 0.0035, G Loss: 6.6959\n",
            "Epoch [1/10], Step [6080/32413], D Loss: 0.0065, G Loss: 6.2090\n",
            "Epoch [1/10], Step [6090/32413], D Loss: 0.0026, G Loss: 7.2874\n",
            "Epoch [1/10], Step [6100/32413], D Loss: 0.0020, G Loss: 6.9488\n",
            "Epoch [1/10], Step [6110/32413], D Loss: 0.0053, G Loss: 5.8723\n",
            "Epoch [1/10], Step [6120/32413], D Loss: 0.0024, G Loss: 6.5170\n",
            "Epoch [1/10], Step [6130/32413], D Loss: 0.0306, G Loss: 9.3903\n",
            "Epoch [1/10], Step [6140/32413], D Loss: 0.0114, G Loss: 7.6994\n",
            "Epoch [1/10], Step [6150/32413], D Loss: 0.0654, G Loss: 7.4318\n",
            "Epoch [1/10], Step [6160/32413], D Loss: 0.0033, G Loss: 7.0126\n",
            "Epoch [1/10], Step [6170/32413], D Loss: 0.0046, G Loss: 6.2275\n",
            "Epoch [1/10], Step [6180/32413], D Loss: 0.0046, G Loss: 6.2738\n",
            "Epoch [1/10], Step [6190/32413], D Loss: 0.0035, G Loss: 7.9358\n",
            "Epoch [1/10], Step [6200/32413], D Loss: 0.0046, G Loss: 6.0814\n",
            "Epoch [1/10], Step [6210/32413], D Loss: 0.0040, G Loss: 7.0807\n",
            "Epoch [1/10], Step [6220/32413], D Loss: 0.0025, G Loss: 6.9592\n",
            "Epoch [1/10], Step [6230/32413], D Loss: 0.0062, G Loss: 5.5682\n",
            "Epoch [1/10], Step [6240/32413], D Loss: 0.0034, G Loss: 6.6782\n",
            "Epoch [1/10], Step [6250/32413], D Loss: 0.0072, G Loss: 5.8760\n",
            "Epoch [1/10], Step [6260/32413], D Loss: 0.0019, G Loss: 6.6994\n",
            "Epoch [1/10], Step [6270/32413], D Loss: 0.0042, G Loss: 6.5010\n",
            "Epoch [1/10], Step [6280/32413], D Loss: 0.0044, G Loss: 6.3043\n",
            "Epoch [1/10], Step [6290/32413], D Loss: 0.0027, G Loss: 7.1208\n",
            "Epoch [1/10], Step [6300/32413], D Loss: 0.0013, G Loss: 7.2822\n",
            "Epoch [1/10], Step [6310/32413], D Loss: 0.0024, G Loss: 6.6010\n",
            "Epoch [1/10], Step [6320/32413], D Loss: 0.0049, G Loss: 5.9988\n",
            "Epoch [1/10], Step [6330/32413], D Loss: 0.0033, G Loss: 5.9931\n",
            "Epoch [1/10], Step [6340/32413], D Loss: 0.0011, G Loss: 7.2593\n",
            "Epoch [1/10], Step [6350/32413], D Loss: 0.0016, G Loss: 7.0049\n",
            "Epoch [1/10], Step [6360/32413], D Loss: 0.0025, G Loss: 6.5542\n",
            "Epoch [1/10], Step [6370/32413], D Loss: 0.0008, G Loss: 7.7329\n",
            "Epoch [1/10], Step [6380/32413], D Loss: 0.0011, G Loss: 7.3046\n",
            "Epoch [1/10], Step [6390/32413], D Loss: 0.0011, G Loss: 6.9679\n",
            "Epoch [1/10], Step [6400/32413], D Loss: 0.0011, G Loss: 7.3604\n",
            "Epoch [1/10], Step [6410/32413], D Loss: 0.0010, G Loss: 7.2676\n",
            "Epoch [1/10], Step [6420/32413], D Loss: 0.0010, G Loss: 7.4939\n",
            "Epoch [1/10], Step [6430/32413], D Loss: 0.0015, G Loss: 7.0633\n",
            "Epoch [1/10], Step [6440/32413], D Loss: 0.0016, G Loss: 7.0066\n",
            "Epoch [1/10], Step [6450/32413], D Loss: 0.0003, G Loss: 8.4672\n",
            "Epoch [1/10], Step [6460/32413], D Loss: 0.0012, G Loss: 7.8262\n",
            "Epoch [1/10], Step [6470/32413], D Loss: 0.0010, G Loss: 7.7476\n",
            "Epoch [1/10], Step [6480/32413], D Loss: 0.0018, G Loss: 7.0263\n",
            "Epoch [1/10], Step [6490/32413], D Loss: 0.0015, G Loss: 7.3180\n",
            "Epoch [1/10], Step [6500/32413], D Loss: 0.0037, G Loss: 7.5126\n",
            "Epoch [1/10], Step [6510/32413], D Loss: 0.0014, G Loss: 8.0434\n",
            "Epoch [1/10], Step [6520/32413], D Loss: 0.0049, G Loss: 6.8551\n",
            "Epoch [1/10], Step [6530/32413], D Loss: 0.0012, G Loss: 7.2203\n",
            "Epoch [1/10], Step [6540/32413], D Loss: 0.0033, G Loss: 6.6282\n",
            "Epoch [1/10], Step [6550/32413], D Loss: 0.0025, G Loss: 6.8165\n",
            "Epoch [1/10], Step [6560/32413], D Loss: 0.0005, G Loss: 10.0971\n",
            "Epoch [1/10], Step [6570/32413], D Loss: 0.0017, G Loss: 8.2184\n",
            "Epoch [1/10], Step [6580/32413], D Loss: 0.0010, G Loss: 7.5865\n",
            "Epoch [1/10], Step [6590/32413], D Loss: 0.0008, G Loss: 7.4492\n",
            "Epoch [1/10], Step [6600/32413], D Loss: 0.0304, G Loss: 4.9371\n",
            "Epoch [1/10], Step [6610/32413], D Loss: 0.0026, G Loss: 6.5496\n",
            "Epoch [1/10], Step [6620/32413], D Loss: 0.0356, G Loss: 6.7703\n",
            "Epoch [1/10], Step [6630/32413], D Loss: 0.0005, G Loss: 8.3839\n",
            "Epoch [1/10], Step [6640/32413], D Loss: 0.0047, G Loss: 6.7116\n",
            "Epoch [1/10], Step [6650/32413], D Loss: 0.0927, G Loss: 4.7429\n",
            "Epoch [1/10], Step [6660/32413], D Loss: 2.7315, G Loss: 5.3767\n",
            "Epoch [1/10], Step [6670/32413], D Loss: 0.2585, G Loss: 2.6445\n",
            "Epoch [1/10], Step [6680/32413], D Loss: 0.0462, G Loss: 4.3980\n",
            "Epoch [1/10], Step [6690/32413], D Loss: 0.0542, G Loss: 3.7826\n",
            "Epoch [1/10], Step [6700/32413], D Loss: 0.4400, G Loss: 2.2821\n",
            "Epoch [1/10], Step [6710/32413], D Loss: 0.0286, G Loss: 4.5940\n",
            "Epoch [1/10], Step [6720/32413], D Loss: 0.0352, G Loss: 4.6042\n",
            "Epoch [1/10], Step [6730/32413], D Loss: 0.0243, G Loss: 5.1815\n",
            "Epoch [1/10], Step [6740/32413], D Loss: 0.1022, G Loss: 5.1174\n",
            "Epoch [1/10], Step [6750/32413], D Loss: 0.0187, G Loss: 6.0889\n",
            "Epoch [1/10], Step [6760/32413], D Loss: 0.0630, G Loss: 4.1499\n",
            "Epoch [1/10], Step [6770/32413], D Loss: 0.0389, G Loss: 4.2543\n",
            "Epoch [1/10], Step [6780/32413], D Loss: 0.0106, G Loss: 8.2639\n",
            "Epoch [1/10], Step [6790/32413], D Loss: 0.0049, G Loss: 6.7323\n",
            "Epoch [1/10], Step [6800/32413], D Loss: 0.0195, G Loss: 5.3371\n",
            "Epoch [1/10], Step [6810/32413], D Loss: 0.0017, G Loss: 7.5864\n",
            "Epoch [1/10], Step [6820/32413], D Loss: 0.0043, G Loss: 4.4834\n",
            "Epoch [1/10], Step [6830/32413], D Loss: 0.5741, G Loss: 6.7721\n",
            "Epoch [1/10], Step [6840/32413], D Loss: 0.0074, G Loss: 5.3224\n",
            "Epoch [1/10], Step [6850/32413], D Loss: 0.0197, G Loss: 5.4559\n",
            "Epoch [1/10], Step [6860/32413], D Loss: 0.0171, G Loss: 4.5751\n",
            "Epoch [1/10], Step [6870/32413], D Loss: 0.0010, G Loss: 7.6237\n",
            "Epoch [1/10], Step [6880/32413], D Loss: 0.0072, G Loss: 5.9938\n",
            "Epoch [1/10], Step [6890/32413], D Loss: 0.0050, G Loss: 7.3116\n",
            "Epoch [1/10], Step [6900/32413], D Loss: 0.0144, G Loss: 5.0372\n",
            "Epoch [1/10], Step [6910/32413], D Loss: 0.0019, G Loss: 6.6000\n",
            "Epoch [1/10], Step [6920/32413], D Loss: 0.0088, G Loss: 11.1468\n",
            "Epoch [1/10], Step [6930/32413], D Loss: 0.0416, G Loss: 4.4016\n",
            "Epoch [1/10], Step [6940/32413], D Loss: 0.6149, G Loss: 0.7907\n",
            "Epoch [1/10], Step [6950/32413], D Loss: 0.8006, G Loss: 2.2573\n",
            "Epoch [1/10], Step [6960/32413], D Loss: 0.1488, G Loss: 3.2214\n",
            "Epoch [1/10], Step [6970/32413], D Loss: 0.3236, G Loss: 2.8114\n",
            "Epoch [1/10], Step [6980/32413], D Loss: 1.2301, G Loss: 4.7231\n",
            "Epoch [1/10], Step [6990/32413], D Loss: 0.2319, G Loss: 3.8841\n",
            "Epoch [1/10], Step [7000/32413], D Loss: 0.0563, G Loss: 4.5946\n",
            "Epoch [1/10], Step [7010/32413], D Loss: 0.1592, G Loss: 4.1598\n",
            "Epoch [1/10], Step [7020/32413], D Loss: 0.0536, G Loss: 2.1593\n",
            "Epoch [1/10], Step [7030/32413], D Loss: 0.3377, G Loss: 2.0289\n",
            "Epoch [1/10], Step [7040/32413], D Loss: 0.0503, G Loss: 3.9128\n",
            "Epoch [1/10], Step [7050/32413], D Loss: 0.0129, G Loss: 4.9792\n",
            "Epoch [1/10], Step [7060/32413], D Loss: 0.0496, G Loss: 4.0763\n",
            "Epoch [1/10], Step [7070/32413], D Loss: 0.0428, G Loss: 8.9050\n",
            "Epoch [1/10], Step [7080/32413], D Loss: 0.0130, G Loss: 5.7636\n",
            "Epoch [1/10], Step [7090/32413], D Loss: 0.2486, G Loss: 7.4062\n",
            "Epoch [1/10], Step [7100/32413], D Loss: 0.0041, G Loss: 5.7710\n",
            "Epoch [1/10], Step [7110/32413], D Loss: 0.0116, G Loss: 7.4996\n",
            "Epoch [1/10], Step [7120/32413], D Loss: 0.0017, G Loss: 7.3841\n",
            "Epoch [1/10], Step [7130/32413], D Loss: 0.0020, G Loss: 7.4582\n",
            "Epoch [1/10], Step [7140/32413], D Loss: 0.0031, G Loss: 6.4743\n",
            "Epoch [1/10], Step [7150/32413], D Loss: 0.0048, G Loss: 6.9216\n",
            "Epoch [1/10], Step [7160/32413], D Loss: 0.1250, G Loss: 4.9875\n",
            "Epoch [1/10], Step [7170/32413], D Loss: 0.0064, G Loss: 8.0687\n",
            "Epoch [1/10], Step [7180/32413], D Loss: 0.0036, G Loss: 7.4828\n",
            "Epoch [1/10], Step [7190/32413], D Loss: 0.0072, G Loss: 5.9156\n",
            "Epoch [1/10], Step [7200/32413], D Loss: 0.0465, G Loss: 4.1183\n",
            "Epoch [1/10], Step [7210/32413], D Loss: 0.0123, G Loss: 4.8980\n",
            "Epoch [1/10], Step [7220/32413], D Loss: 0.0067, G Loss: 6.3894\n",
            "Epoch [1/10], Step [7230/32413], D Loss: 0.0053, G Loss: 6.0833\n",
            "Epoch [1/10], Step [7240/32413], D Loss: 0.0445, G Loss: 4.5753\n",
            "Epoch [1/10], Step [7250/32413], D Loss: 0.0072, G Loss: 5.6159\n",
            "Epoch [1/10], Step [7260/32413], D Loss: 0.0157, G Loss: 6.3691\n",
            "Epoch [1/10], Step [7270/32413], D Loss: 0.0029, G Loss: 6.9439\n",
            "Epoch [1/10], Step [7280/32413], D Loss: 0.0089, G Loss: 6.2921\n",
            "Epoch [1/10], Step [7290/32413], D Loss: 0.0032, G Loss: 6.2500\n",
            "Epoch [1/10], Step [7300/32413], D Loss: 0.0013, G Loss: 7.3319\n",
            "Epoch [1/10], Step [7310/32413], D Loss: 0.0008, G Loss: 7.7991\n",
            "Epoch [1/10], Step [7320/32413], D Loss: 0.0021, G Loss: 6.8125\n",
            "Epoch [1/10], Step [7330/32413], D Loss: 0.0062, G Loss: 6.2037\n",
            "Epoch [1/10], Step [7340/32413], D Loss: 0.0045, G Loss: 6.8615\n",
            "Epoch [1/10], Step [7350/32413], D Loss: 0.0033, G Loss: 6.4851\n",
            "Epoch [1/10], Step [7360/32413], D Loss: 0.0014, G Loss: 7.1824\n",
            "Epoch [1/10], Step [7370/32413], D Loss: 0.0044, G Loss: 7.6268\n",
            "Epoch [1/10], Step [7380/32413], D Loss: 0.0007, G Loss: 7.6439\n",
            "Epoch [1/10], Step [7390/32413], D Loss: 0.0013, G Loss: 7.4841\n",
            "Epoch [1/10], Step [7400/32413], D Loss: 0.0068, G Loss: 6.8785\n",
            "Epoch [1/10], Step [7410/32413], D Loss: 0.0011, G Loss: 7.2678\n",
            "Epoch [1/10], Step [7420/32413], D Loss: 0.0012, G Loss: 7.2212\n",
            "Epoch [1/10], Step [7430/32413], D Loss: 0.0028, G Loss: 7.1788\n",
            "Epoch [1/10], Step [7440/32413], D Loss: 0.0006, G Loss: 8.0588\n",
            "Epoch [1/10], Step [7450/32413], D Loss: 0.0013, G Loss: 7.1027\n",
            "Epoch [1/10], Step [7460/32413], D Loss: 0.0076, G Loss: 7.1190\n",
            "Epoch [1/10], Step [7470/32413], D Loss: 0.0031, G Loss: 6.9712\n",
            "Epoch [1/10], Step [7480/32413], D Loss: 0.0016, G Loss: 7.2218\n",
            "Epoch [1/10], Step [7490/32413], D Loss: 0.0003, G Loss: 8.7156\n",
            "Epoch [1/10], Step [7500/32413], D Loss: 0.0009, G Loss: 7.4359\n",
            "Epoch [1/10], Step [7510/32413], D Loss: 0.0010, G Loss: 7.1941\n",
            "Epoch [1/10], Step [7520/32413], D Loss: 0.0019, G Loss: 6.7167\n",
            "Epoch [1/10], Step [7530/32413], D Loss: 0.0022, G Loss: 8.1678\n",
            "Epoch [1/10], Step [7540/32413], D Loss: 0.0008, G Loss: 7.9448\n",
            "Epoch [1/10], Step [7550/32413], D Loss: 0.0010, G Loss: 7.2679\n",
            "Epoch [1/10], Step [7560/32413], D Loss: 0.0007, G Loss: 7.5505\n",
            "Epoch [1/10], Step [7570/32413], D Loss: 0.0007, G Loss: 7.6992\n",
            "Epoch [1/10], Step [7580/32413], D Loss: 0.0004, G Loss: 8.4864\n",
            "Epoch [1/10], Step [7590/32413], D Loss: 0.0005, G Loss: 8.1690\n",
            "Epoch [1/10], Step [7600/32413], D Loss: 0.0027, G Loss: 7.0443\n",
            "Epoch [1/10], Step [7610/32413], D Loss: 0.0009, G Loss: 7.5554\n",
            "Epoch [1/10], Step [7620/32413], D Loss: 0.0005, G Loss: 8.0991\n",
            "Epoch [1/10], Step [7630/32413], D Loss: 0.0003, G Loss: 8.6796\n",
            "Epoch [1/10], Step [7640/32413], D Loss: 0.0007, G Loss: 7.6749\n",
            "Epoch [1/10], Step [7650/32413], D Loss: 0.0006, G Loss: 7.8895\n",
            "Epoch [1/10], Step [7660/32413], D Loss: 0.0013, G Loss: 7.5652\n",
            "Epoch [1/10], Step [7670/32413], D Loss: 0.0053, G Loss: 6.3647\n",
            "Epoch [1/10], Step [7680/32413], D Loss: 0.0011, G Loss: 7.7616\n",
            "Epoch [1/10], Step [7690/32413], D Loss: 0.0012, G Loss: 7.1027\n",
            "Epoch [1/10], Step [7700/32413], D Loss: 0.0011, G Loss: 7.6599\n",
            "Epoch [1/10], Step [7710/32413], D Loss: 0.0014, G Loss: 6.9495\n",
            "Epoch [1/10], Step [7720/32413], D Loss: 0.0009, G Loss: 7.8689\n",
            "Epoch [1/10], Step [7730/32413], D Loss: 0.0006, G Loss: 7.8819\n",
            "Epoch [1/10], Step [7740/32413], D Loss: 0.0007, G Loss: 7.7043\n",
            "Epoch [1/10], Step [7750/32413], D Loss: 0.0004, G Loss: 8.3089\n",
            "Epoch [1/10], Step [7760/32413], D Loss: 0.0005, G Loss: 8.1274\n",
            "Epoch [1/10], Step [7770/32413], D Loss: 0.0005, G Loss: 8.2572\n",
            "Epoch [1/10], Step [7780/32413], D Loss: 0.0005, G Loss: 8.4941\n",
            "Epoch [1/10], Step [7790/32413], D Loss: 0.0010, G Loss: 7.3042\n",
            "Epoch [1/10], Step [7800/32413], D Loss: 0.0019, G Loss: 6.7359\n",
            "Epoch [1/10], Step [7810/32413], D Loss: 0.0012, G Loss: 7.1805\n",
            "Epoch [1/10], Step [7820/32413], D Loss: 0.0011, G Loss: 7.1695\n",
            "Epoch [1/10], Step [7830/32413], D Loss: 0.0020, G Loss: 6.9775\n",
            "Epoch [1/10], Step [7840/32413], D Loss: 0.1437, G Loss: 3.6939\n",
            "Epoch [1/10], Step [7850/32413], D Loss: 0.0011, G Loss: 7.4711\n",
            "Epoch [1/10], Step [7860/32413], D Loss: 0.0027, G Loss: 6.4836\n",
            "Epoch [1/10], Step [7870/32413], D Loss: 0.0016, G Loss: 7.3066\n",
            "Epoch [1/10], Step [7880/32413], D Loss: 0.0029, G Loss: 6.6591\n",
            "Epoch [1/10], Step [7890/32413], D Loss: 0.0016, G Loss: 7.9068\n",
            "Epoch [1/10], Step [7900/32413], D Loss: 0.0283, G Loss: 6.6349\n",
            "Epoch [1/10], Step [7910/32413], D Loss: 0.0029, G Loss: 6.7003\n",
            "Epoch [1/10], Step [7920/32413], D Loss: 0.0008, G Loss: 8.1508\n",
            "Epoch [1/10], Step [7930/32413], D Loss: 0.0331, G Loss: 5.5536\n",
            "Epoch [1/10], Step [7940/32413], D Loss: 13.0545, G Loss: 6.4047\n",
            "Epoch [1/10], Step [7950/32413], D Loss: 0.4278, G Loss: 2.5959\n",
            "Epoch [1/10], Step [7960/32413], D Loss: 0.2044, G Loss: 2.8946\n",
            "Epoch [1/10], Step [7970/32413], D Loss: 0.1023, G Loss: 3.4144\n",
            "Epoch [1/10], Step [7980/32413], D Loss: 0.0192, G Loss: 5.5888\n",
            "Epoch [1/10], Step [7990/32413], D Loss: 0.0240, G Loss: 4.4488\n",
            "Epoch [1/10], Step [8000/32413], D Loss: 0.0258, G Loss: 4.5504\n",
            "Epoch [1/10], Step [8010/32413], D Loss: 0.0166, G Loss: 7.0250\n",
            "Epoch [1/10], Step [8020/32413], D Loss: 0.0127, G Loss: 5.0322\n",
            "Epoch [1/10], Step [8030/32413], D Loss: 0.0308, G Loss: 4.7117\n",
            "Epoch [1/10], Step [8040/32413], D Loss: 0.0116, G Loss: 5.6378\n",
            "Epoch [1/10], Step [8050/32413], D Loss: 0.0220, G Loss: 4.7201\n",
            "Epoch [1/10], Step [8060/32413], D Loss: 0.0124, G Loss: 4.9166\n",
            "Epoch [1/10], Step [8070/32413], D Loss: 0.0251, G Loss: 4.7650\n",
            "Epoch [1/10], Step [8080/32413], D Loss: 0.0040, G Loss: 6.8855\n",
            "Epoch [1/10], Step [8090/32413], D Loss: 0.0127, G Loss: 5.0889\n",
            "Epoch [1/10], Step [8100/32413], D Loss: 0.0039, G Loss: 6.1837\n",
            "Epoch [1/10], Step [8110/32413], D Loss: 0.0016, G Loss: 7.1590\n",
            "Epoch [1/10], Step [8120/32413], D Loss: 0.0028, G Loss: 6.4496\n",
            "Epoch [1/10], Step [8130/32413], D Loss: 0.0118, G Loss: 8.0159\n",
            "Epoch [1/10], Step [8140/32413], D Loss: 0.0021, G Loss: 8.0871\n",
            "Epoch [1/10], Step [8150/32413], D Loss: 0.0071, G Loss: 5.7154\n",
            "Epoch [1/10], Step [8160/32413], D Loss: 0.0003, G Loss: 8.2868\n",
            "Epoch [1/10], Step [8170/32413], D Loss: 0.0006, G Loss: 8.3730\n",
            "Epoch [1/10], Step [8180/32413], D Loss: 0.0016, G Loss: 7.1504\n",
            "Epoch [1/10], Step [8190/32413], D Loss: 0.0025, G Loss: 6.4617\n",
            "Epoch [1/10], Step [8200/32413], D Loss: 0.2161, G Loss: 3.5621\n",
            "Epoch [1/10], Step [8210/32413], D Loss: 0.0522, G Loss: 3.9304\n",
            "Epoch [1/10], Step [8220/32413], D Loss: 0.0177, G Loss: 4.6063\n",
            "Epoch [1/10], Step [8230/32413], D Loss: 0.1656, G Loss: 4.0764\n",
            "Epoch [1/10], Step [8240/32413], D Loss: 0.0027, G Loss: 6.9757\n",
            "Epoch [1/10], Step [8250/32413], D Loss: 0.4258, G Loss: 5.8485\n",
            "Epoch [1/10], Step [8260/32413], D Loss: 0.0061, G Loss: 7.0079\n",
            "Epoch [1/10], Step [8270/32413], D Loss: 0.0112, G Loss: 5.3150\n",
            "Epoch [1/10], Step [8280/32413], D Loss: 0.0197, G Loss: 5.3315\n",
            "Epoch [1/10], Step [8290/32413], D Loss: 0.0032, G Loss: 6.0925\n",
            "Epoch [1/10], Step [8300/32413], D Loss: 0.4610, G Loss: 8.5590\n",
            "Epoch [1/10], Step [8310/32413], D Loss: 0.0105, G Loss: 7.4921\n",
            "Epoch [1/10], Step [8320/32413], D Loss: 0.0128, G Loss: 4.9843\n",
            "Epoch [1/10], Step [8330/32413], D Loss: 0.0030, G Loss: 6.7632\n",
            "Epoch [1/10], Step [8340/32413], D Loss: 0.0624, G Loss: 4.6532\n",
            "Epoch [1/10], Step [8350/32413], D Loss: 0.0082, G Loss: 5.7486\n",
            "Epoch [1/10], Step [8360/32413], D Loss: 0.0092, G Loss: 5.7112\n",
            "Epoch [1/10], Step [8370/32413], D Loss: 0.1652, G Loss: 6.6060\n",
            "Epoch [1/10], Step [8380/32413], D Loss: 0.8414, G Loss: 6.6584\n",
            "Epoch [1/10], Step [8390/32413], D Loss: 0.0165, G Loss: 4.6703\n",
            "Epoch [1/10], Step [8400/32413], D Loss: 0.0170, G Loss: 4.7745\n",
            "Epoch [1/10], Step [8410/32413], D Loss: 0.2823, G Loss: 3.2133\n",
            "Epoch [1/10], Step [8420/32413], D Loss: 0.1157, G Loss: 3.8192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ],
      "metadata": {
        "id": "ul-93rAGQwV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "#Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=4),  # Output a single channel\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = torch.mean(x, dim=[2, 3])  # Global Average Pooling to reduce spatial dimensions\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "save_dir = \"/content/\"\n",
        "pic_dir = \"/content/181107.png_output_image.png\"\n",
        "epoch = 1\n",
        "\n",
        "# Load the saved state dictionaries\n",
        "with open(os.path.join(save_dir, f\"Icon_generator_epoch_{epoch}.pth\"), 'rb') as f:\n",
        "    state_dict = torch.load(f, map_location=device)\n",
        "generator.load_state_dict(state_dict)\n",
        "\n",
        "with open(os.path.join(save_dir, f\"Icon_discriminator_epoch_{epoch}.pth\"), 'rb') as f:\n",
        "    state_dict = torch.load(f, map_location=device)\n",
        "discriminator.load_state_dict(state_dict)\n",
        "# Set models to evaluation mode (if needed)\n",
        "generator.eval()\n",
        "discriminator.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCD-f7NhQuFr",
        "outputId": "d5a944e4-798c-436a-b189-7a0bb280048c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-7829f2b3eafc>:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f, map_location=device)\n",
            "<ipython-input-4-7829f2b3eafc>:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f, map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00eLXjoVTlj8",
        "outputId": "3a4e72c8-0d6c-4602-a87c-6fb765d5fbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "181107.png_output_image.png\tIcon_generator_epoch_1.pth\n",
            "Icon_discriminator_epoch_1.pth\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: display result from this model by using 181107.png_output_image.png as input image.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "# Assuming the model and image are in the /content directory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the saved generator model\n",
        "generator = Generator().to(device)\n",
        "save_dir = \"/content/\"\n",
        "epoch = 1\n",
        "with open(os.path.join(save_dir, f\"Icon_generator_epoch_{epoch}.pth\"), 'rb') as f:\n",
        "    state_dict = torch.load(f, map_location=device)\n",
        "generator.load_state_dict(state_dict)\n",
        "generator.eval()\n",
        "\n",
        "# Load and preprocess the input image\n",
        "img_path = \"/content/181107.png_output_image.png\"\n",
        "# img_path = \"/content/acorn-line-drawing-9.jpg\"\n",
        "try:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    with torch.no_grad():\n",
        "        output_tensor = generator(img_tensor)\n",
        "\n",
        "    # Post-process the output tensor\n",
        "    output_image = output_tensor.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0)\n",
        "    output_image = ((output_image + 1) * 127.5).astype(np.uint8)\n",
        "    output_image = Image.fromarray(output_image)\n",
        "    output_image.save(\"colored_icon_from_saved_model.png\")\n",
        "    print(\"Colored icon saved as 'colored_icon_from_saved_model.png'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Image file not found at {img_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw5zXKHXT_jA",
        "outputId": "bf46c223-3600-4d92-aa7f-684e71ab4415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colored icon saved as 'colored_icon_from_saved_model.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-cfc4ed4db0ce>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f, map_location=device)\n"
          ]
        }
      ]
    }
  ]
}